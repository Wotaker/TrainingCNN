{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:GlobalAsyncCheckpointManager is not imported correctly. Checkpointing of GlobalDeviceArrays will not be available.To use the feature, install tensorstore.\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable, Tuple, Any, Dict, List\n",
    "from absl import logging\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.random import PRNGKey as jkey\n",
    "from chex import Scalar, Array, PRNGKey, Shape\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from flax.training.train_state import TrainState as RawTrainState\n",
    "from flax.training.checkpoints import restore_checkpoint\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from training_cnn import get_CIFAR10, Metrices, checkpoint\n",
    "from architectures import *\n",
    "\n",
    "logging.set_verbosity(logging.WARN)\n",
    "\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading CIFAR10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = get_CIFAR10(jkey(SEED), 1.0)\n",
    "\n",
    "first_n = 8\n",
    "dummy_batch = x_train[:first_n]\n",
    "dummy_labels = y_train[:first_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState(RawTrainState):\n",
    "    batch_stats: flax.core.FrozenDict\n",
    "\n",
    "\n",
    "def conv_block_with_bn(x: Array, features: int, training: bool) -> Array:\n",
    "\n",
    "    x = nn.Conv(features=features, kernel_size=(3, 3), padding='SAME')(x)\n",
    "    x = nn.BatchNorm(use_running_average=not training)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Conv(features=features, kernel_size=(3, 3), padding='SAME')(x)\n",
    "    x = nn.BatchNorm(use_running_average=not training)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "class BatchNormCNN(nn.Module):\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, batch: Array, training: bool):\n",
    "        \n",
    "        batch_size = batch.shape[0]\n",
    "        x = batch / 255\n",
    "        x = conv_block_with_bn(x, 20, training)\n",
    "        x = conv_block_with_bn(x, 40, training)\n",
    "        x = conv_block_with_bn(x, 80, training)\n",
    "        x = conv_block_with_bn(x, 160, training)\n",
    "        x = jnp.reshape(x, (batch_size, -1))\n",
    "        x = nn.Dense(features=10)(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "def create_BatchNormCNN(\n",
    "    dummy_batch: Array,\n",
    "    init_key: PRNGKey,\n",
    "    lr: Scalar = 0.001,\n",
    "    momentum: Scalar = 0.9\n",
    ") -> TrainState:\n",
    "\n",
    "    cnn = BatchNormCNN()\n",
    "    variables = cnn.init(init_key, dummy_batch, training=False)\n",
    "\n",
    "    return TrainState.create(\n",
    "        apply_fn=cnn.apply,\n",
    "        params=variables['params'],\n",
    "        tx=optax.sgd(learning_rate=lr, momentum=momentum),\n",
    "        batch_stats=variables['batch_stats'])\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def eval_BatchNormCNN(state: TrainState, batch: Array, labels: Array):\n",
    "    \n",
    "    logits = BatchNormCNN().apply(\n",
    "        {'params': state.params, 'batch_stats': state.batch_stats},\n",
    "        batch,\n",
    "        training=False\n",
    "    )\n",
    "    one_hot = jax.nn.one_hot(labels, 10)\n",
    "\n",
    "    accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "    loss = jnp.mean(optax.softmax_cross_entropy(logits=logits, labels=one_hot))\n",
    "\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def apply_model(state: TrainState, batch: Array, labels: Array):\n",
    "    \"\"\"Computes gradients, loss and accuracy for a single batch.\"\"\"\n",
    "\n",
    "    def loss_fn(params, batch_stats):\n",
    "\n",
    "        logits, batch_stats = state.apply_fn(\n",
    "            {'params': params, 'batch_stats': batch_stats},\n",
    "            batch,\n",
    "            training=True,\n",
    "            mutable=['batch_stats']\n",
    "        )\n",
    "        one_hot = jax.nn.one_hot(labels, 10)\n",
    "        loss = jnp.mean(optax.softmax_cross_entropy(logits=logits, labels=one_hot))\n",
    "\n",
    "        return loss, (logits, batch_stats)\n",
    "\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, (logits, batch_stats)), grads = grad_fn(state.params, state.batch_stats)\n",
    "    accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "    \n",
    "    return grads, loss, accuracy\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update_model(state: TrainState, grads: nn.FrozenDict):\n",
    "\n",
    "    return state.apply_gradients(grads=grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "\tstate: TrainState,\n",
    "\tx_train: Array,\n",
    "\ty_train: Array,\n",
    "\tbatch_size: int,\n",
    "\tperm_key: PRNGKey\n",
    ") -> Tuple[TrainState, Scalar, Scalar]:\n",
    "\n",
    "\tn_samples = x_train.shape[0]\n",
    "\tsteps_per_epoch = n_samples // batch_size\n",
    "\n",
    "\tperms = jax.random.permutation(perm_key, n_samples)[:steps_per_epoch * batch_size]\n",
    "\tperms = jnp.reshape(perms, (steps_per_epoch, batch_size))\n",
    "\n",
    "\tepoch_loss = []\n",
    "\tepoch_accuracy = []\n",
    "\n",
    "\tfor perm in perms:\n",
    "\n",
    "\t\tx_batch = x_train[perm, ...]\n",
    "\t\ty_batch = y_train[perm, ...]\n",
    "\t\tgrads, loss, accuracy = apply_model(state, x_batch, y_batch)\n",
    "\t\tstate = update_model(state, grads)\n",
    "\t\tepoch_loss.append(loss)\n",
    "\t\tepoch_accuracy.append(accuracy)\n",
    "\t\n",
    "\treturn state, np.mean(epoch_loss), np.mean(epoch_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(\n",
    "    seed: int,\n",
    "\tepochs: int,\n",
    "\tbatch_size: int,\n",
    "\tcreate_state_fun: Callable,\n",
    "\tlr: Scalar = 0.001,\n",
    "\tmomentum: Scalar = 0.9,\n",
    "\tds_chunk_size = 1.0,\n",
    "\tlog_every: int = 0,\n",
    "    checkpoint_dir: str = \"\",\n",
    ") -> Tuple[TrainState, Metrices, float]:\n",
    "\n",
    "    # Create PRNG keys\n",
    "    key = jkey(seed)\n",
    "    key, ds_key, init_key = jax.random.split(key, 3)\n",
    "\n",
    "    # Load CIFAE10 dataset\n",
    "    (x_train, y_train), (x_test, y_test) = get_CIFAR10(ds_key, chunk_size=ds_chunk_size)\n",
    "\n",
    "    # Create structures to accumulate metrices\n",
    "    metrices = Metrices(epochs)\n",
    "\n",
    "    # Get initial MinCNN training state\n",
    "    state = create_state_fun(x_train, init_key, lr=lr, momentum=momentum)\n",
    "\n",
    "    # Iterate through the dataset for epochs number of times\n",
    "    start = time.time()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        key, epoch_key = jax.random.split(key)\n",
    "        state, train_loss, train_accuracy = train_epoch(state, x_train, y_train, batch_size, epoch_key)\n",
    "        test_loss, test_accuracy = eval_BatchNormCNN(state, x_test, y_test)\n",
    "        metrices.update(train_loss, train_accuracy * 100, test_loss, test_accuracy * 100)\n",
    "        \n",
    "        if log_every and (epoch % log_every == 0 or epoch in {1, epochs}):\n",
    "            print(\n",
    "                'epoch:% 3d, train_loss: %.4f, train_accuracy: %.2f, test_loss: %.4f, test_accuracy: %.2f'\n",
    "                % (epoch, train_loss, train_accuracy * 100, test_loss, test_accuracy * 100)\n",
    "            )\n",
    "        \n",
    "        checkpoint(checkpoint_dir, state, metrices, epoch, time.time() - start)\n",
    "\n",
    "    return state, metrices, time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1, train_loss: 1.3804, train_accuracy: 50.33, test_loss: 2.2957, test_accuracy: 13.71\n",
      "epoch:  2, train_loss: 0.9800, train_accuracy: 65.12, test_loss: 2.2902, test_accuracy: 12.83\n",
      "epoch:  3, train_loss: 0.7898, train_accuracy: 72.38, test_loss: 2.2879, test_accuracy: 11.44\n",
      "epoch:  4, train_loss: 0.6582, train_accuracy: 76.94, test_loss: 2.2892, test_accuracy: 11.34\n",
      "epoch:  5, train_loss: 0.5481, train_accuracy: 80.96, test_loss: 2.2849, test_accuracy: 13.11\n",
      "Total training time: 364.011\n"
     ]
    }
   ],
   "source": [
    "final_state, metrices, elapsed_time = train_and_eval(\n",
    "    seed=42,\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    "    create_state_fun=create_BatchNormCNN,\n",
    "    lr=0.001,\n",
    "    momentum=0.9,\n",
    "    ds_chunk_size=1.0,\n",
    "    log_every=1,\n",
    "    checkpoint_dir=os.path.join(\"checkpoints/batch_norm_cnn\"),\n",
    ")\n",
    "print(f\"Total training time: {elapsed_time:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, init_key, epoch_key = jax.random.split(jkey(42), 3)\n",
    "state = create_BatchNormCNN(x_train, init_key)\n",
    "\n",
    "state, train_loss, train_accuracy = train_epoch(state, x_train, y_train, 32, epoch_key)\n",
    "loss, accuracy = eval_BatchNormCNN(state, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pyncn310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54934a5afaccb2ddc951324a0bd74860167bceb796717f7926bef290ae4125ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
